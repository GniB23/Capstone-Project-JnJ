{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def deEmoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def text_preprocess(text):\n",
    "    text = re.sub(r'#', '', text) #Replace the # symbol with '' in every tweet\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text) #Replace hyperlinks with '' in every tweet\n",
    "    text = re.sub(r'[^\\w\\d\\s]+', '', text) #Remove all punctuations\n",
    "    text = deEmoji(text) #Remove emoji\n",
    "    text = re.sub(r'\\n', '', text) #Remove \\n\n",
    "    \n",
    "    #Remove all stopwords\n",
    "    stopwords_eng = stopwords.words('english')\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords_eng) + r')\\b\\s*')\n",
    "    text = pattern.sub('', text)\n",
    "    \n",
    "    #Apply stem\n",
    "    ps = PorterStemmer()\n",
    "    tk = TweetTokenizer(preserve_case=False, strip_handles = True)\n",
    "    tweet_tokens = tk.tokenize(text)\n",
    "    tweet_finish = ''\n",
    "    for token in tweet_tokens:\n",
    "        stem = ps.stem(token)\n",
    "        tweet_finish += (stem + ' ')\n",
    "    return tweet_finish  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[['medical_device', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['medical_device', 'text'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(tweets.text)\n",
    "for i in range(length):\n",
    "    tweets.text.iloc[i] = text_preprocess(tweets.text.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_device</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>yall know i know im never leav boy alon jnj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>superkelli 24 such funni scene jnj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>jampjohnson flagform confirm continu possibl j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>jnj daili rsi hasnt touch 69 sinc februari 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>we outstand opportun join non clinic safeti te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6489</th>\n",
       "      <td>False</td>\n",
       "      <td>bought jnj call 904 155 53 entri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6490</th>\n",
       "      <td>False</td>\n",
       "      <td>wikileak pamper huggi johnson johnson jnj hist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6491</th>\n",
       "      <td>False</td>\n",
       "      <td>wikileak pamper huggi johnson johnson jnj hist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>False</td>\n",
       "      <td>hostess_snack jnj kick theyr kick get done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>False</td>\n",
       "      <td>superkelli 24 i hope rc give jnj amp johnsonbr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6494 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      medical_device                                               text\n",
       "0              False       yall know i know im never leav boy alon jnj \n",
       "1              False                superkelli 24 such funni scene jnj \n",
       "2              False  jampjohnson flagform confirm continu possibl j...\n",
       "3              False  jnj daili rsi hasnt touch 69 sinc februari 202...\n",
       "4              False  we outstand opportun join non clinic safeti te...\n",
       "...              ...                                                ...\n",
       "6489           False                  bought jnj call 904 155 53 entri \n",
       "6490           False  wikileak pamper huggi johnson johnson jnj hist...\n",
       "6491           False  wikileak pamper huggi johnson johnson jnj hist...\n",
       "6492           False        hostess_snack jnj kick theyr kick get done \n",
       "6493           False  superkelli 24 i hope rc give jnj amp johnsonbr...\n",
       "\n",
       "[6494 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    5859\n",
      "True      635\n",
      "Name: medical_device, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x=tweets['medical_device'].value_counts()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(x=\"medical_device\", data=tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec #Word2Vec is mostly used for huge datasets\n",
    "\n",
    "# create Word2vec model\n",
    "#here words_f should be a list containing words from each document. say 1st row of the list is words from the 1st document/sentence\n",
    "#length of words_f is number of documents/sentences in your dataset\n",
    "tweets['clean_text_tok']=[nltk.word_tokenize(i) for i in tweets['text']] #convert preprocessed sentence to tokenized sentence\n",
    "model = Word2Vec(tweets['clean_text_tok'],min_count=1)  #min_count=1 means word should be present at least across all documents,\n",
    "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "\n",
    "\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  #combination of word and its vector\n",
    "\n",
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(tweets[\"text\"].values.reshape(-1,1), tweets[\"medical_device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res = pd.Series(X_res.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res,\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True, \n",
    "                                                    #stratify=tweets[\"medical_device\"],\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     4697\n",
       "False    4677\n",
       "Name: medical_device, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
    "X_test_tok= [nltk.word_tokenize(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimators:  {'C': 10000.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.98      0.99      1182\n",
      "        True       0.98      1.00      0.99      1162\n",
      "\n",
      "    accuracy                           0.99      2344\n",
      "   macro avg       0.99      0.99      0.99      2344\n",
      "weighted avg       0.99      0.99      0.99      2344\n",
      "\n",
      "Confusion Matrix: [[1162   20]\n",
      " [   0 1162]]\n",
      "AUC: 0.9975485699141746\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr_param = {'solver' : ['liblinear'], 'C':np.logspace(-4, 4, 20),'penalty' : ['l2']}\n",
    "lr_tfidf= GridSearchCV(LogisticRegression(), lr_param)\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    " \n",
    "print('Best estimators: ', lr_tfidf.best_params_)\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR (w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimators:  {'C': 3792.690190732246, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.85      0.86      1182\n",
      "        True       0.85      0.87      0.86      1162\n",
      "\n",
      "    accuracy                           0.86      2344\n",
      "   macro avg       0.86      0.86      0.86      2344\n",
      "weighted avg       0.86      0.86      0.86      2344\n",
      "\n",
      "Confusion Matrix: [[1005  177]\n",
      " [ 149 1013]]\n",
      "AUC: 0.9374743353399093\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
    "lr_w2v_param = {'solver' : ['liblinear'], 'C':np.logspace(-4, 4, 20),'penalty' : ['l2']}\n",
    "\n",
    "lr_w2v=GridSearchCV(LogisticRegression(), lr_w2v_param)\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    "\n",
    "print('Best estimators: ', lr_w2v.best_params_)\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimators:  {'alpha': 0.0001}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.96      0.98      1182\n",
      "        True       0.96      1.00      0.98      1162\n",
      "\n",
      "    accuracy                           0.98      2344\n",
      "   macro avg       0.98      0.98      0.98      2344\n",
      "weighted avg       0.98      0.98      0.98      2344\n",
      "\n",
      "Confusion Matrix: [[1137   45]\n",
      " [   1 1161]]\n",
      "AUC: 0.9971772514277559\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "#It's a probabilistic classifier that makes use of Bayes' Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.\n",
    "\n",
    "nb_param = {'alpha': [0.0001,0.001,0.1, 1]}\n",
    "nb_tfidf = GridSearchCV(MultinomialNB(), nb_param)\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = nb_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    " \n",
    "print('Best estimators: ', nb_tfidf.best_params_)\n",
    "    \n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.97      0.99      1182\n",
      "        True       0.97      1.00      0.99      1162\n",
      "\n",
      "    accuracy                           0.99      2344\n",
      "   macro avg       0.99      0.99      0.99      2344\n",
      "weighted avg       0.99      0.99      0.99      2344\n",
      "\n",
      "Confusion Matrix: [[1151   31]\n",
      " [   0 1162]]\n",
      "AUC: 0.9969049512043823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbdt_tfidf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.2, max_depth=15, random_state=42)\n",
    "gbdt_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = gbdt_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = gbdt_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    " \n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.98      0.99      1182\n",
      "        True       0.98      1.00      0.99      1162\n",
      "\n",
      "    accuracy                           0.99      2344\n",
      "   macro avg       0.99      0.99      0.99      2344\n",
      "weighted avg       0.99      0.99      0.99      2344\n",
      "\n",
      "Confusion Matrix: [[1157   25]\n",
      " [   0 1162]]\n",
      "AUC: 0.9955543712194682\n"
     ]
    }
   ],
   "source": [
    "gbdt_w2v = GradientBoostingClassifier(n_estimators=100, learning_rate=0.2, max_depth=15, random_state=42)\n",
    "gbdt_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = gbdt_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob = gbdt_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    " \n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:57:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.98      0.99      1182\n",
      "        True       0.98      1.00      0.99      1162\n",
      "\n",
      "    accuracy                           0.99      2344\n",
      "   macro avg       0.99      0.99      0.99      2344\n",
      "weighted avg       0.99      0.99      0.99      2344\n",
      "\n",
      "Confusion Matrix: [[1154   28]\n",
      " [   1 1161]]\n",
      "AUC: 0.9968401524881251\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_tfidf = XGBClassifier(n_estimators=100, learning_rate=0.2, max_depth=15, random_state=42)\n",
    "xgb_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = xgb_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = xgb_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    " \n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:57:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.97      0.98      1182\n",
      "        True       0.97      1.00      0.98      1162\n",
      "\n",
      "    accuracy                           0.98      2344\n",
      "   macro avg       0.98      0.98      0.98      2344\n",
      "weighted avg       0.98      0.98      0.98      2344\n",
      "\n",
      "Confusion Matrix: [[1142   40]\n",
      " [   0 1162]]\n",
      "AUC: 0.9973206822940784\n"
     ]
    }
   ],
   "source": [
    "xgb_w2v = XGBClassifier(n_estimators=100, learning_rate=0.2, max_depth=15, random_state=42)\n",
    "xgb_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = xgb_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob = xgb_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    " \n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
